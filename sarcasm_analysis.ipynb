{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarcasm Detection for Steam Game Reviews\n",
    "\n",
    "This notebook implements a sarcasm detection system for Steam game reviews using:\n",
    "1. Traditional ML (Naive Bayes with TF-IDF features)\n",
    "2. BERT-based deep learning model\n",
    "3. Gaming-specific feature engineering\n",
    "\n",
    "The system analyzes review text, helpfulness votes, funny votes, and playtime to detect sarcastic reviews in the Steam gaming context. Features include:\n",
    "- Text-based features (TF-IDF, n-grams)\n",
    "- Gaming-specific patterns\n",
    "- Sentiment analysis\n",
    "- Behavioral signals (funny votes, helpful votes)\n",
    "- Text style analysis (capitalization, punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple Metal (MPS) for GPU acceleration.\n",
      "Selected device: mps\n",
      "‚úÖ All packages imported and environment is set up!\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# PyTorch and Transformers imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data quietly\n",
    "for resource in ['stopwords', 'punkt', 'vader_lexicon']:\n",
    "    try:\n",
    "        nltk.download(resource, quiet=True)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not download NLTK resource: {resource}. Error: {e}\")\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# --- GPU Setup for Metal (M1/M2/M3), CUDA, or CPU ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    torch.mps.manual_seed(RANDOM_SEED)\n",
    "    print(\"üöÄ Using Apple Metal (MPS) for GPU acceleration.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    print(\"üöÄ Using NVIDIA CUDA for GPU acceleration.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è GPU not available, using CPU. Training will be slower.\")\n",
    "\n",
    "print(f\"Selected device: {device}\")\n",
    "print(\"‚úÖ All packages imported and environment is set up!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading dataset from 'steam_reviews.csv'...\n",
      "‚úÖ Successfully loaded and preprocessed dataset: 19931 reviews\n",
      "\n",
      "üìä Sentiment Distribution:\n",
      "sentiment_label\n",
      "1    14005\n",
      "0     5926\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_data(file_path='steam_reviews.csv'):\n",
    "    \"\"\"\n",
    "    Loads the Steam dataset and creates sentiment labels from the 'recommendation' column.\n",
    "    \"\"\"\n",
    "    print(f\"üì• Loading dataset from '{file_path}'...\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå Error: Dataset file not found at '{file_path}'.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # --- Preprocessing and Label Creation ---\n",
    "        # 1: Positive (Recommended), 0: Negative (Not Recommended)\n",
    "        df['sentiment_label'] = (df['recommendation'].str.strip() == 'Recommended').astype(int)\n",
    "        df = df.dropna(subset=['review']) # Drop rows with no review text\n",
    "        df['review'] = df['review'].astype(str)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded and preprocessed dataset: {df.shape[0]} reviews\")\n",
    "        print(\"\\nüìä Sentiment Distribution:\")\n",
    "        print(df['sentiment_label'].value_counts())\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Execute Data Loading ---\n",
    "df_labeled = load_and_preprocess_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß© Preparing dataset from 19931 reviews...\n",
      "‚úÖ Final prepared dataset size: 10000\n",
      "üìà Final distribution:\n",
      "sentiment_label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset_for_training(df, target_size=10000, positive_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Cleans, samples, and balances the dataset for training.\n",
    "    \"\"\"\n",
    "    if df is None: return None\n",
    "    \n",
    "    print(f\"\\nüß© Preparing dataset from {len(df)} reviews...\")\n",
    "    \n",
    "    df = df[df['review'].str.len() > 20] # Remove very short reviews\n",
    "    \n",
    "    positive_reviews = df[df['sentiment_label'] == 1]\n",
    "    negative_reviews = df[df['sentiment_label'] == 0]\n",
    "        \n",
    "    # Balance the dataset by sampling\n",
    "    positive_target = int(target_size * positive_ratio)\n",
    "    negative_target = target_size - positive_target\n",
    "    \n",
    "    positive_sample = positive_reviews.sample(n=min(positive_target, len(positive_reviews)), random_state=RANDOM_SEED)\n",
    "    negative_sample = negative_reviews.sample(n=min(negative_target, len(negative_reviews)), random_state=RANDOM_SEED)\n",
    "    \n",
    "    df_prepared = pd.concat([positive_sample, negative_sample]).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ Final prepared dataset size: {len(df_prepared)}\")\n",
    "    print(f\"üìà Final distribution:\\n{df_prepared['sentiment_label'].value_counts()}\")\n",
    "    \n",
    "    return df_prepared\n",
    "\n",
    "# --- Execute Data Preparation ---\n",
    "if 'df_labeled' in locals():\n",
    "    df_prepared = prepare_dataset_for_training(df_labeled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SentimentDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for sentiment classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ SentimentDataset class defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Starting Model Training and Evaluation...\n",
      "\n",
      "üìä Training Logistic Regression model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 98\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# --- Execute the entire training pipeline ---\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_prepared\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[0;32m---> 98\u001b[0m     final_results \u001b[38;5;241m=\u001b[39m train_and_evaluate_models(df_prepared)\n",
      "Cell \u001b[0;32mIn[21], line 21\u001b[0m, in \u001b[0;36mtrain_and_evaluate_models\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     18\u001b[0m X_train_tfidf \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[1;32m     19\u001b[0m X_test_tfidf \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m---> 21\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m LogisticRegression(random_state\u001b[38;5;241m=\u001b[39mRANDOM_SEED, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     22\u001b[0m lr_model\u001b[38;5;241m.\u001b[39mfit(X_train_tfidf, y_train)\n\u001b[1;32m     23\u001b[0m y_pred_lr \u001b[38;5;241m=\u001b[39m lr_model\u001b[38;5;241m.\u001b[39mpredict(X_test_tfidf)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_models(df):\n",
    "    \"\"\"\n",
    "    Orchestrates the training and evaluation of both Logistic Regression and BERT models.\n",
    "    \"\"\"\n",
    "    if df is None: return None\n",
    "    print(\"\\nü§ñ Starting Model Training and Evaluation...\")\n",
    "    \n",
    "    X = df['review'].values\n",
    "    y = df['sentiment_label'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # --- 1. Train Logistic Regression with TF-IDF ---\n",
    "    print(\"\\nüìä Training Logistic Regression model...\")\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    lr_model = LogisticRegression(random_state=RANDOM_SEED, max_iter=1000)\n",
    "    lr_model.fit(X_train_tfidf, y_train)\n",
    "    y_pred_lr = lr_model.predict(X_test_tfidf)\n",
    "    \n",
    "    results['Logistic Regression'] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "        'F1-Score': f1_score(y_test, y_pred_lr),\n",
    "        'Precision': precision_score(y_test, y_pred_lr),\n",
    "        'Recall': recall_score(y_test, y_pred_lr)\n",
    "    }\n",
    "    print(\"‚úÖ Logistic Regression training complete.\")\n",
    "\n",
    "    # --- 2. Train BERT ---\n",
    "    print(\"\\nüî• Training BERT model...\")\n",
    "    bert_model, bert_tokenizer, bert_stats = train_bert_function(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    bert_model.eval()\n",
    "    y_pred_bert = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(X_test, desc=\"Evaluating BERT\"):\n",
    "            inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
    "            outputs = bert_model(**inputs)\n",
    "            y_pred_bert.append(torch.argmax(outputs.logits).item())\n",
    "            \n",
    "    results['BERT'] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_bert),\n",
    "        'F1-Score': f1_score(y_test, y_pred_bert),\n",
    "        'Precision': precision_score(y_test, y_pred_bert),\n",
    "        'Recall': recall_score(y_test, y_pred_bert)\n",
    "    }\n",
    "\n",
    "    # --- Display Final Results ---\n",
    "    print(\"\\n\\nüìä FINAL RESULTS COMPARISON\")\n",
    "    results_df = pd.DataFrame(results).round(4)\n",
    "    print(results_df)\n",
    "    \n",
    "    best_model_name = results_df.loc['F1-Score'].idxmax()\n",
    "    print(f\"\\nüèÜ Best Model based on F1-Score: {best_model_name}\")\n",
    "    \n",
    "    return {\n",
    "        'results_df': results_df,\n",
    "        'bert_stats': bert_stats,\n",
    "        'y_test': y_test,\n",
    "        'y_pred_lr': y_pred_lr,\n",
    "        'y_pred_bert': y_pred_bert,\n",
    "        'best_model_name': best_model_name\n",
    "    }\n",
    "\n",
    "def train_bert_function(train_texts, train_labels, val_texts, val_labels):\n",
    "    \"\"\"A self-contained function to train the BERT model.\"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "    \n",
    "    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    print(\"Training BERT...\")\n",
    "    for epoch in range(2): # Keep training short for demonstration\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device),\n",
    "                labels=batch['labels'].to(device)\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    return model, tokenizer, [] # Return empty stats for simplicity here\n",
    "\n",
    "# --- Execute the entire training pipeline ---\n",
    "if 'df_prepared' in locals():\n",
    "    final_results = train_and_evaluate_models(df_prepared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(df):\n",
    "    \"\"\"\n",
    "    Train and evaluate Naive Bayes and BERT models for sarcasm detection\n",
    "    \"\"\"\n",
    "    print(\"\\nü§ñ Training models...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df['review'].values\n",
    "    y = (df['sarcasm_score'] > 2.0).astype(int)  # Binary classification: sarcastic vs non-sarcastic\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Train Naive Bayes with TF-IDF\n",
    "    print(\"\\nTraining Naive Bayes model...\")\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    nb_model = MultinomialNB(alpha=0.1)\n",
    "    nb_model.fit(X_train_tfidf, y_train)\n",
    "    nb_pred = nb_model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Calculate Naive Bayes metrics\n",
    "    nb_accuracy = accuracy_score(y_test, nb_pred)\n",
    "    nb_f1 = f1_score(y_test, nb_pred)\n",
    "    nb_precision = precision_score(y_test, nb_pred)\n",
    "    nb_recall = recall_score(y_test, nb_pred)\n",
    "    \n",
    "    results['Naive Bayes'] = {\n",
    "        'Accuracy': nb_accuracy,\n",
    "        'F1-Score': nb_f1,\n",
    "        'Precision': nb_precision,\n",
    "        'Recall': nb_recall\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Naive Bayes Results:\")\n",
    "    print(f\"   Accuracy: {nb_accuracy:.3f}\")\n",
    "    print(f\"   F1-Score: {nb_f1:.3f}\")\n",
    "    print(f\"   Precision: {nb_precision:.3f}\")\n",
    "    print(f\"   Recall: {nb_recall:.3f}\")\n",
    "    \n",
    "    # 2. Train BERT\n",
    "    print(\"\\nTraining BERT model...\")\n",
    "    bert_model, bert_tokenizer = train_bert_model(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test\n",
    "    )\n",
    "    \n",
    "    # Display final results\n",
    "    print(\"\\nüìä FINAL RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    results_df = pd.DataFrame(results).round(3)\n",
    "    print(results_df)\n",
    "    \n",
    "    # Find best model\n",
    "    best_model = results_df.loc['F1-Score'].idxmax()\n",
    "    best_f1 = results_df.loc['F1-Score', best_model]\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Model: {best_model} (F1-Score: {best_f1:.3f})\")\n",
    "    \n",
    "    return {\n",
    "        'naive_bayes': {\n",
    "            'model': nb_model,\n",
    "            'vectorizer': vectorizer\n",
    "        },\n",
    "        'bert': {\n",
    "            'model': bert_model,\n",
    "            'tokenizer': bert_tokenizer\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_for_training(df, target_size=1200):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for training with balanced sampling\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        print(\"‚ùå No data available for training\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìä Preparing dataset from {len(df)} reviews...\")\n",
    "    \n",
    "    # Clean the data\n",
    "    df = df.dropna(subset=['review'])\n",
    "    df = df[df['review'].str.len() > 10]  # Remove very short reviews\n",
    "    \n",
    "    # Sample data if we have too many reviews\n",
    "    if len(df) > target_size:\n",
    "        print(f\"üìâ Sampling {target_size} reviews from {len(df)} total...\")\n",
    "        \n",
    "        # Try to maintain balance between sarcastic and genuine\n",
    "        if 'label' in df.columns:\n",
    "            sarcastic_target = int(target_size * 0.65)  # 65% sarcastic\n",
    "            genuine_target = target_size - sarcastic_target\n",
    "            \n",
    "            sarcastic_reviews = df[df['label'] == 1]\n",
    "            genuine_reviews = df[df['label'] == 0]\n",
    "            \n",
    "            # Sample from each group\n",
    "            if len(sarcastic_reviews) >= sarcastic_target:\n",
    "                sarcastic_sample = sarcastic_reviews.sample(n=sarcastic_target, random_state=42)\n",
    "            else:\n",
    "                sarcastic_sample = sarcastic_reviews\n",
    "            \n",
    "            if len(genuine_reviews) >= genuine_target:\n",
    "                genuine_sample = genuine_reviews.sample(n=genuine_target, random_state=42)\n",
    "            else:\n",
    "                genuine_sample = genuine_reviews\n",
    "            \n",
    "            df = pd.concat([sarcastic_sample, genuine_sample]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.sample(n=target_size, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ Final dataset: {len(df)} reviews\")\n",
    "    if 'sentiment' in df.columns:\n",
    "        print(f\"üìà Final distribution: {df['sentiment'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmAwarePreprocessor:\n",
    "    \"\"\"\n",
    "    Advanced text preprocessor specifically designed for Steam review sarcasm detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        # Preserve negation and intensifier words\n",
    "        self.preserve_words = {\n",
    "            'not', 'no', 'never', 'nothing', 'nobody', 'nowhere', 'neither', 'nor', \n",
    "            'barely', 'hardly', 'scarcely', 'very', 'so', 'extremely', 'totally', \n",
    "            'absolutely', 'completely', 'quite', 'rather', 'really', 'definitely',\n",
    "            'certainly', 'obviously', 'clearly', 'surely'\n",
    "        }\n",
    "        self.stop_words = self.stop_words - self.preserve_words\n",
    "        \n",
    "        # Steam/Gaming specific terms to preserve\n",
    "        self.gaming_terms = {\n",
    "            'dlc', 'fps', 'gameplay', 'multiplayer', 'singleplayer', 'coop', 'pvp',\n",
    "            'respawn', 'checkpoint', 'save', 'load', 'crash', 'bug', 'glitch', \n",
    "            'lag', 'ping', 'server', 'patch', 'update', 'nerf', 'buff', 'op',\n",
    "            'ragequit', 'noob', 'pro', 'speedrun', 'achievements', 'trophies'\n",
    "        }\n",
    "    \n",
    "    def extract_advanced_sarcasm_features(self, text):\n",
    "        \"\"\"\n",
    "        Extract comprehensive sarcasm detection features\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        \n",
    "        features = {}\n",
    "        text_lower = text.lower()\n",
    "        words = text.split()\n",
    "        \n",
    "        # Basic text statistics\n",
    "        features['word_count'] = len(words)\n",
    "        features['char_count'] = len(text)\n",
    "        features['avg_word_length'] = np.mean([len(word) for word in words]) if words else 0\n",
    "        features['sentence_count'] = len(re.split(r'[.!?]+', text))\n",
    "        \n",
    "        # Capitalization patterns (often used in sarcasm)\n",
    "        features['caps_ratio'] = sum(1 for c in text if c.isupper()) / len(text) if text else 0\n",
    "        features['caps_words'] = sum(1 for word in words if word.isupper())\n",
    "        features['caps_sequences'] = len(re.findall(r'[A-Z]{3,}', text))\n",
    "        \n",
    "        # Punctuation patterns (excessive punctuation in sarcasm)\n",
    "        features['exclamation_count'] = text.count('!')\n",
    "        features['question_count'] = text.count('?')\n",
    "        features['ellipsis_count'] = len(re.findall(r'\\.{3,}', text))\n",
    "        features['multiple_punct'] = len(re.findall(r'[!?]{2,}', text))\n",
    "        features['punct_density'] = sum([features['exclamation_count'], features['question_count'], \n",
    "                                       features['ellipsis_count']]) / len(text) if text else 0\n",
    "        \n",
    "        # Sentiment analysis scores\n",
    "        sentiment_scores = analyzer.polarity_scores(text)\n",
    "        features.update({f'vader_{k}': v for k, v in sentiment_scores.items()})\n",
    "        \n",
    "        # Advanced sarcasm indicators\n",
    "        features['intensifier_count'] = sum(1 for word in text_lower.split() \n",
    "                                          if word in ['very', 'so', 'extremely', 'totally', 'absolutely', \n",
    "                                                    'completely', 'quite', 'rather', 'really', 'definitely'])\n",
    "        \n",
    "        # Contradiction patterns\n",
    "        positive_words = ['great', 'amazing', 'excellent', 'fantastic', 'wonderful', 'perfect', \n",
    "                         'brilliant', 'outstanding', 'superb', 'best', 'love', 'awesome']\n",
    "        negative_words = ['terrible', 'awful', 'horrible', 'worst', 'hate', 'broken', 'crash', \n",
    "                         'bug', 'glitch', 'frustrating', 'annoying', 'impossible']\n",
    "        \n",
    "        features['positive_word_count'] = sum(1 for word in positive_words if word in text_lower)\n",
    "        features['negative_word_count'] = sum(1 for word in negative_words if word in text_lower)\n",
    "        features['pos_neg_cooccurrence'] = 1 if features['positive_word_count'] > 0 and features['negative_word_count'] > 0 else 0\n",
    "        \n",
    "        # Steam-specific sarcasm patterns\n",
    "        steam_sarcasm_patterns = [\n",
    "            r'10/10.*would.*(?:rage|quit|crash|never)',\n",
    "            r'perfect.*(?:if you enjoy|for people who)',\n",
    "            r'great.*(?:if you like|when you)',\n",
    "            r'love.*how.*(?:crash|bug|glitch)',\n",
    "            r'amazing.*(?:graphics|sound).*(?:from|like).*(?:199\\d|200\\d)',\n",
    "            r'excellent.*(?:tutorial|help).*(?:hours|still|never)',\n",
    "            r'who needs.*(?:tutorial|help|balance)',\n",
    "            r'nothing like.*getting.*(?:owned|destroyed|rekt)',\n",
    "            r'highly recommend.*(?:if you|for people who).*(?:enjoy|like).*(?:pain|suffering|frustration)'\n",
    "        ]\n",
    "        \n",
    "        features['steam_sarcasm_count'] = sum(1 for pattern in steam_sarcasm_patterns \n",
    "                                            if re.search(pattern, text_lower))\n",
    "        \n",
    "        # Gaming terminology\n",
    "        features['gaming_terms_count'] = sum(1 for term in self.gaming_terms if term in text_lower)\n",
    "        \n",
    "        # Quotation marks (often used sarcastically)\n",
    "        features['quote_count'] = text.count('\"') + text.count(\"'\")\n",
    "        \n",
    "        # All caps words (emphasis in sarcasm)\n",
    "        features['all_caps_words'] = len(re.findall(r'\\b[A-Z]{2,}\\b', text))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Comprehensive text preprocessing for sarcasm detection\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        \n",
    "        # Extract features before preprocessing\n",
    "        features = self.extract_advanced_sarcasm_features(text)\n",
    "        \n",
    "        # Preserve important patterns before cleaning\n",
    "        text = re.sub(r'(\\d+)/(\\d+)', r'RATING_\\1_\\2', text)  # Preserve ratings like 10/10\n",
    "        text = re.sub(r'[.]{3,}', ' ELLIPSIS ', text)\n",
    "        text = re.sub(r'[!]{2,}', ' MULTIPLE_EXCLAMATION ', text)\n",
    "        text = re.sub(r'[?]{2,}', ' MULTIPLE_QUESTION ', text)\n",
    "        \n",
    "        # Clean URLs, mentions, etc.\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "        \n",
    "        # Preserve negation context\n",
    "        negation_patterns = [\n",
    "            (r'\\b(not|no|never|nothing|nobody|nowhere|neither|nor)\\s+(\\w+)', r'\\1_\\2'),\n",
    "            (r'\\b(barely|hardly|scarcely)\\s+(\\w+)', r'\\1_\\2'),\n",
    "            (r'\\b(can\\'t|cannot|won\\'t|wouldn\\'t|shouldn\\'t|couldn\\'t)\\s+(\\w+)', r'\\1_\\2'),\n",
    "            (r'\\b(don\\'t|doesn\\'t|didn\\'t)\\s+(\\w+)', r'\\1_\\2')\n",
    "        ]\n",
    "        \n",
    "        for pattern, replacement in negation_patterns:\n",
    "            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Tokenize and clean\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        \n",
    "        # Remove stopwords but preserve important words\n",
    "        tokens = [token for token in tokens \n",
    "                 if token not in self.stop_words or token in self.preserve_words or token in self.gaming_terms]\n",
    "        \n",
    "        # Keep alphabetic tokens and preserved special tokens\n",
    "        special_tokens = ['ELLIPSIS', 'MULTIPLE_EXCLAMATION', 'MULTIPLE_QUESTION'] + [f'RATING_{i}_{j}' for i in range(1, 11) for j in range(1, 11)]\n",
    "        tokens = [token for token in tokens \n",
    "                 if token.isalpha() or token in special_tokens or '_' in token]\n",
    "        \n",
    "        clean_text = ' '.join(tokens)\n",
    "        \n",
    "        return clean_text, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution pipeline for real Steam reviews sarcasm detection\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting Real Steam Reviews Sarcasm Detection Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Load real dataset\n",
    "    df_raw = load_real_steam_dataset()\n",
    "    \n",
    "    if df_raw is None:\n",
    "        print(\"‚ùå Could not load dataset. Please check your setup.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìä Raw dataset loaded: {df_raw.shape}\")\n",
    "    print(f\"Columns: {list(df_raw.columns)}\")\n",
    "    \n",
    "    # Step 2: Identify sarcastic reviews\n",
    "    df_labeled = identify_sarcastic_reviews(df_raw)\n",
    "    \n",
    "    # Step 3: Prepare for training\n",
    "    df = prepare_dataset_for_training(df_labeled, target_size=1200)\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"‚ùå Could not prepare dataset for training.\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Advanced preprocessing\n",
    "    print(\"\\nüîß Applying advanced preprocessing...\")\n",
    "    preprocessor = SarcasmAwarePreprocessor()\n",
    "    \n",
    "    results = df['review'].apply(lambda x: preprocessor.preprocess_text(x))\n",
    "    df['clean_review'] = [result[0] for result in results]\n",
    "    features_list = [result[1] for result in results]\n",
    "    \n",
    "    # Convert features to DataFrame\n",
    "    features_df = pd.DataFrame(features_list)\n",
    "    df = pd.concat([df, features_df], axis=1)\n",
    "    \n",
    "    print(f\"‚úÖ Preprocessing completed!\")\n",
    "    print(f\"Added {len(features_df.columns)} advanced features\")\n",
    "    \n",
    "    # Step 5: Create feature matrix\n",
    "    print(\"\\nüéØ Creating feature matrices...\")\n",
    "    \n",
    "    # TF-IDF features\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=3000,\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "    \n",
    "    X_tfidf = tfidf.fit_transform(df['clean_review'])\n",
    "    \n",
    "    # Numerical features\n",
    "    feature_cols = [col for col in features_df.columns if isinstance(df[col].iloc[0], (int, float))]\n",
    "    X_numerical = df[feature_cols].fillna(0)\n",
    "    \n",
    "    # Combine features\n",
    "    from scipy.sparse import hstack\n",
    "    import scipy.sparse as sp\n",
    "    X_combined = hstack([X_tfidf, sp.csr_matrix(X_numerical.values)])\n",
    "    \n",
    "    y = df['label'].values\n",
    "    \n",
    "    print(f\"üìà Feature matrix shape: {X_combined.shape}\")\n",
    "    print(f\"üè∑Ô∏è Label distribution: Sarcastic: {sum(y)}, Genuine: {len(y) - sum(y)}\")\n",
    "    \n",
    "    # Step 6: Train models\n",
    "    print(\"\\nü§ñ Training models...\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_combined, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    models = {\n",
    "        'Naive Bayes': MultinomialNB(alpha=0.1)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'F1-Score': f1,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ {name} - Accuracy: {accuracy:.3f}, F1: {f1:.3f}\")\n",
    "    \n",
    "    # Step 7: Display results\n",
    "    print(\"\\nüìä FINAL RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    results_df = pd.DataFrame(results).round(3)\n",
    "    print(results_df)\n",
    "    \n",
    "    # Find best model\n",
    "    best_model = results_df.loc['F1-Score'].idxmax()\n",
    "    best_f1 = results_df.loc['F1-Score', best_model]\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "    print(f\"üéØ Best F1 Score: {best_f1}\")\n",
    "    \n",
    "    # Step 8: Show example predictions\n",
    "    print(\"\\nüîç Example Predictions:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    best_model_obj = models[best_model]\n",
    "    sample_indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        prediction = best_model_obj.predict(X_test[idx])[0]\n",
    "        actual = y_test[idx]\n",
    "        review_text = df.iloc[X_test.indices[idx] if hasattr(X_test, 'indices') else idx]['review'][:100]\n",
    "        \n",
    "        print(f\"Review: {review_text}...\")\n",
    "        print(f\"Predicted: {'Sarcastic' if prediction == 1 else 'Genuine'}\")\n",
    "        print(f\"Actual: {'Sarcastic' if actual == 1 else 'Genuine'}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\nüéâ Analysis completed successfully!\")\n",
    "    \n",
    "    return df, results_df, models[best_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting Sarcasm Detection Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Load and preprocess data\n",
    "    df_raw = load_real_steam_dataset()\n",
    "    if df_raw is None:\n",
    "        print(\"‚ùå Failed to load dataset\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # Step 2: Identify sarcastic reviews\n",
    "    df_labeled = identify_sarcastic_reviews(df_raw)\n",
    "    \n",
    "    # Step 3: Prepare balanced dataset\n",
    "    df_prepared = prepare_dataset_for_training(df_labeled)\n",
    "    if df_prepared is None:\n",
    "        print(\"‚ùå Failed to prepare dataset\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Step 4: Train and evaluate models\n",
    "    results = train_and_evaluate_models(df_prepared, use_features=True)\n",
    "    \n",
    "    # Step 5: Save the best model\n",
    "    best_model_name = results['results'].loc['F1-Score'].idxmax()\n",
    "    if best_model_name == 'BERT':\n",
    "        best_model = results['bert']['model']\n",
    "        tokenizer = results['bert']['tokenizer']\n",
    "        \n",
    "        # Save BERT model and tokenizer\n",
    "        save_dir = 'best_model'\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "            \n",
    "        best_model.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"\\n‚úÖ Best model (BERT) saved to {save_dir}/\")\n",
    "    else:\n",
    "        # Save Naive Bayes model and vectorizer\n",
    "        import joblib\n",
    "        joblib.dump(results['naive_bayes']['model'], 'best_model/naive_bayes_model.joblib')\n",
    "        joblib.dump(results['naive_bayes']['vectorizer'], 'best_model/tfidf_vectorizer.joblib')\n",
    "        print(\"\\n‚úÖ Best model (Naive Bayes) saved to best_model/\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\nüìä Final Model Performance:\")\n",
    "    print(results['results'])\n",
    "    \n",
    "    print(\"\\nüéâ Pipeline completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
